---
title: "Exploratory Data Analysis (EDA) Sentiment Analysis"
subtitle: "Data science portfolio"
author: "Manuel Alejandro Matías Astorga"
date: "2024"
output: 
  # prettydoc::html_pretty:
  html_document:
    theme: yeti
    highlight: tango
    toc: true
    toc_depth: 3
    number_sections: true
    toc_float: false
    df_print: paged
    keep_md: true
# runtime: shiny
---

# Preparation of the work environment

In this section, we will prepare our working environment by installing and loading the necessary libraries. Proper setup is essential for efficient data handling, visualization, and analysis in R. This step ensures that all the tools and libraries we need are readily available for our exploration and modeling tasks.

## Installing libraries

In this step, we install the necessary libraries for our analysis, including packages for data manipulation, visualization, text mining, and word clouds. Each package serves a unique purpose in the data science workflow, as detailed below:

-   **tidyverse**: For data manipulation and visualization.
-   **data.table**: Optimized for handling large datasets efficiently.
-   **ggplot2**: Essential for creating a wide variety of visualizations.
-   **dplyr**: Used for data wrangling and filtering.
-   **tm**: Text mining for natural language processing (NLP).
-   **wordcloud**: Generates word clouds for text data visualization.
-   **readr**: Efficient data reading.
-   **knitr**: Supports notebook creation and reporting.

```{r installing packages, eval=FALSE}
# Install necessary libraries (only first time)
# install.packages(c("tidyverse", "data.table", "ggplot2", "dplyr", "tm", "wordcloud", "knitr", "tm"))
```

## Loading libraries

With the libraries installed, we now load them into our R environment. Each library plays a crucial role in the exploratory data analysis (EDA) and text mining process, as outlined below. Some libraries may output messages upon loading, so we suppress these to keep the notebook clean and readable.

```{r loading libraries, message=FALSE, warning=FALSE}
# Load libraries for EDA and text mining
library(tidyverse)   # Data manipulation and visualization
library(data.table)  # Efficient data handling for large datasets
library(tidytext)    # Text mining for NLP tasks
library(ggplot2)     # Data visualization
library(scales)      # For formatting scales in ggplot2
library(dplyr)       # Data wrangling
library(stopwords)   # Stopwords for text analysis
library(tibble)      # Data manipulation
library(SnowballC)   # Stemming for text analysis
library(udpipe)      # Lemmatization for text analysis
library(caret)       # Machine learning models
library(Matrix)      # Sparse matrix for text analysis
library(tm)          # Text mining for NLP tasks
library(wordcloud)   # Word cloud visualization
library(readr)       # Data reading
library(knitr)       # For creating notebooks 
library(DT)          # For interactive data tables
library(stringr)     # For string manipulation
library(shiny)       # For interactive plots
```

After loading the libraries, we confirm that they have loaded successfully by displaying the versions of key libraries. This ensures compatibility and reproducibility of the analysis.

```{r check libraries, comment=""}
# Print version of main libraries
cat("Loaded tidyverse version:", as.character(packageVersion("tidyverse")), "\n")
cat("Loaded data.table version:", as.character(packageVersion("data.table")), "\n")
cat("Loaded ggplot2 version:", as.character(packageVersion("ggplot2")), "\n")
```

## Loading Data {.tabset .tabset-fade .tabset-pills}

### IMDB Dataset

The **IMDB Dataset** contains 50,000 movie reviews labeled with binary sentiments (positive or negative). This dataset is useful for natural language processing (NLP) tasks, particularly sentiment analysis. It includes: - **25,000 reviews for training** and **25,000 reviews for testing**, making it a benchmark dataset for sentiment classification tasks. - Each review is labeled with a **sentiment** value (positive or negative) in order to train and evaluate machine learning models.

The dataset was downloaded from the [Kaggle repository](https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews).

<h4>Dataset Structure</h4>

Each row in the dataset represents a movie review with two main columns:

-   **review**: The text of the movie review.
-   **sentiment**: The label for the sentiment, either "positive" or "negative".

**Example**:

| review | sentiment |
|------------------------------------|:------------------------------------:|
| "One of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked." | positive |
| "A wonderful little production. The filming technique is very unassuming- very old-time-B..." | positive |

We will start by loading the dataset using the `read_csv` function from `readr`.

```{r loading data, message=FALSE, warning=FALSE}
df <- read_csv("../src/data/IMDB Dataset.csv",)
```


<h3>Initial Inspection of Data</h3>

To better understand the dataset, we start by displaying the first few rows using `datatable(df)`. This gives us a quick overview of the text data format in the `review` column and the labels in the `sentiment` column.

**NOTE: TO INTERACT WITH THE TABLE, WE HAVE USED `datatable`, BESIDES WE HAVE RENDERED THEIR HTML TAGS TO HAVE A BETTER LOOK OF THE REVIEWS**

```{r data inspection, warning=FALSE}

# Create a interactive table with the package DT
datatable(df, filter = 'top', options = list(pageLength = 6, autoWidth = TRUE), escape = FALSE)

# filter: creates the boxes at the TOP of the columns to find elements in the dataset
# options:
# - pageLenght: how many elements of the dataset are shown
# - autoWidth: TRUE adjust the width of each column to their content.
# escape: FALSE to show the reviews rendering their HTML tags.
```

1.  **Data Format**:

-   The dataset is presented as a `datatable` with **6 rows** and **2 columns**: `review` and `sentiment.` Each row represents a single movie review and its corresponding sentiment label. As a dynamic table from the package `DT`, it can be handled to check each review.

2.  **Columns**:

-   `review`: This column contains text data, where each entry is a movie review. Observations from these initial rows reveal:
    -   Reviews are written in **natural language**, exhibiting varied lengths and formats.
    -   There is some **HTML-like syntax** present, such as `<br />`, but they are rendered by using `escape = FALSE` in the snippet code above in `datatable()`.
    -   Reviews contain **punctuation** and **special characters** (e.g., "), which also suggests that further text processing steps, like removing or standardizing punctuation, could be beneficial.
-   `sentiment`: This column contains categorical labels indicating the sentiment associated with each review:
    -   “**positive**”: Suggests the review expresses a favorable opinion about the movie.
    -   “**negative**”: Suggests the review expresses an unfavorable opinion.
    -   In this preview (the first 6 reviews), 5 of the 6 reviews are labeled as “positive” and 1 as “negative”. However, the actual distribution will need to be confirmed in the full dataset.

3.  **Observations on Sample Reviews**:

-   From this small sample, we can observe the following patterns:
    -   Reviews labeled “positive” often use expressive language that conveys enjoyment or admiration.
    -   The presence of both “positive” and “negative” labels suggests a balanced dataset, although this balance will need to be validated in the following sentiment distribution analysis.
-   Based on these initial findings, preprocessing steps such as HTML tag removal, punctuation handling, and standardization of text may be necessary before proceeding with sentiment analysis.

This preliminary examination confirms that the dataset is suitable for a binary sentiment classification task, with review as the text input and sentiment as the target variable. Additionally, it highlights the need for cleaning and standardizing the text data as part of the preprocessing pipeline.

<h3>Structural Overview and Basic Descriptive Statistics</h3>

Let's take a look at the structure of the dataset, which provides an overview of the column types and a summary of the dataset's dimensions. This initial check also helps confirm that the `sentiment` column is a character variable, which we will later convert to binary format for statistical modeling.

```{r data structure, comment=""}
# Check structure of the dataset
str(df)
```

The output of `str(df)` reveals the following information about the dataset:

-   **Data Type**: The dataset is a `spec_tbl_df`, a specialized type of tibble provided by the readr package, which allows for efficient handling and display of large data.
-   **Dimensions**: The dataset contains **50,000** rows and **2 columns**.
-   **Columns**:
    -   `review`: This column is of type `character`, meaning it contains text data. Each entry in this column represents a movie review. Due to their extension, they are separated by the "| __truncated__ " expression.
    -   `sentiment`: This column is also of type `character.` It contains the sentiment label for each review, either “positive” or “negative”.
-   **Attributes**:
    -   `spec`: This attribute indicates that `review` and `sentiment` are recognized as `character` columns, which aligns with our expectations.
    -   `problems`: An attribute that detects any potential parsing issues when loading the data. Since it’s empty, no parsing issues were encountered.

This structure confirms that the dataset is suitable for a sentiment analysis task, where `review` serves as the input text and sentiment as the binary target variable.

```{r data summary, comment=""}
# Check summary of the dataset
summary(df)
```

Since both `review` and `sentiment` are character columns, `summary(df)` provides basic information on their lengths and classes. Here’s a breakdown:

-   `review`: Contains 50,000 entries of type `character`, as expected for text data. This confirms that each entry represents an individual review without any missing values.
-   `sentiment`: Also contains 50,000 entries of type `character.` Since it aligns with the dataset documentation, this suggests there are no missing labels for sentiment.

The absence of missing values indicates that the dataset is complete and does not require any initial imputation. This allows us to proceed directly with the exploratory data analysis (EDA) and text preprocessing steps.

<h3>Cleaning the text!</h3>

Let's see how the text data looks like before and after cleaning. Before, we used `datatable()` to display the first few rows of the dataset, which don't showed the presence of HTML tags in the reviews due to they were rendered. As a better reference of the dataset, let's use `tibble` to show the data as it is showed in the `.csv` file.



```{r data inspection before cleaning - html tags, warning=FALSE}
# Create a static table with the package tibble (included in tidyverse)
print(as_tibble(df), n = 5)
```



1. **Removing HTML tags**:

Before to proceed with the EDA, we need to clean a little bit our data. Besides the data has not missing values, it has a lot of HTML tags, special characters and uppercase letters. We will convert the reviews into lowercase and remove the HTML tags and special characters. We can use `stringr` to do this.

```{r cleaning text - html tags, warning=FALSE}
# Cleaning the review column
df$review <- stringr::str_replace_all(df$review, "<.*?>", "")
```

- `<.*?>` is the regular expression that matches any HTML tag and is replaced by an empty string `""`.
  - `<` matches the character `<` literally (case sensitive).
  - `.*?` matches any character (except for line terminators) as few times as possible. The `?` makes the `.*` lazy (or not greedy), so it matches as few characters as possible. This is important to avoid removing the entire text between two tags.
  - `>` matches the character `>` literally (case sensitive).
  
**NOTE: IF WE'D MISSED THE `?` IN `.*?`, IT WOULD HAVE BEEN GREEDY AND WOULD HAVE REMOVED THE ENTIRE TEXT BETWEEN THE FIRST `<` AND THE LAST `>` IN THE STRING.**

The above code uses the `str_replace_all` function from the `stringr` package to remove all HTML tags from the `review` column, `"<.*?>"` is a regular expression that matches any HTML tag and is replaced by an empty string `""`. This step is essential for cleaning the text data and preparing it for further analysis.

```{r data inspection after cleaning - html tags, warning=FALSE}
# Create a static table with the package tibble (included in tidyverse)
print(as_tibble(df), n = 5)
```

2. **Removing special characters and punctuation**

But as we can see in the 5th row, there are still some special characters in the reviews, like the escape character `\"` due to the reviews in this dataset are just between `""`. We can remove these special characters using the `str_replace_all()` function.

```{r cleaning text - special characters, warning=FALSE}
# Remove special characters and punctuation
df$review <- stringr::str_replace_all(df$review, "[^[:alnum:]\\s]", "")
```

- `[^[:alnum:]\\s]` is the regular expression that matches any character that is not a letter, digit, or whitespace. 
  - The `^` inside the square brackets `[]` negates the character class, meaning it matches any character that is not in the specified class. 
  - The `[:alnum:]` class matches any alphanumeric character (letters and digits), while 
  - `\\s` matches any whitespace character (space, tab, newline), it has double `\` cause `\s` has a special meaning in the strings in R, so it needs to be escaped.

The code above uses the `str_replace_all` function with the regular expression `"[^[:alnum:]\\s]"` to remove all special characters and punctuation from the `review` column. Now let's check the data again to see the changes.

```{r data inspection after cleaning - special characters, warning=FALSE}
# Create a static table with the package tibble (included in tidyverse)
print(as_tibble(df), n = 5)
```

3. **Converting to lowercase**

Next step is to convert all the text to lowercase and remove special characters. So we will use the `tolower()` function and the `str_replace_all()` function to remove all special characters.

```{r cleaning text - lowercase and special characters, warning=FALSE}
# Convert the review column to lowercase
df$review <- tolower(df$review)
```

and then, the database looks like this

```{r data inspection after cleaning - lowercase, warning=FALSE}
# Create a static table with the package tibble (included in tidyverse)
print(as_tibble(df), n = 5)
```

4. **Remove Extra Whitespaces**

To this point, our database looks pretty good, but may have some extra whitespaces that we can remove to have a more uniform text. We can use the `str_squish()` function from the `stringr` package to remove extra whitespaces.

```{r data inspection - after clean, warning=FALSE}
# Create a interactive table with the package DT
datatable(df, filter = 'top', options = list(pageLength = 3, autoWidth = TRUE), escape = FALSE)
```

To remove extra whitespaces to obtains a more uniform text, we use

```{r}
# Remove extra whitespaces
df$review <- stringr::str_squish(df$review)
```

so we have

```{r data inspection after cleaning - whitespaces, warning=FALSE}
# Create a static table with the package tibble (included in tidyverse)
print(as_tibble(df), n = 5)
```

5. **Tokenization**

We have treated our database to be analyzed, but we may need to tokenize the reviews to analyze the words. We can use the `unnest_tokens()` function from the `tidytext` package to tokenize the reviews.

```{r tokenization, warning=FALSE}
# Tokenize text into words
tokenized_reviews <- df %>%
  unnest_tokens(word, review)
```

and then, we can see the first 5 rows of the tokenized reviews

<a id="tokenization"></a>
```{r data inspection after tokenization, warning=FALSE}
# Create a static table with the package tibble (included in tidyverse)
print(as_tibble(tokenized_reviews), n = 5)
```

The tokenization process has split each review into individual words, creating a new dataset with one word per row. This format is suitable for further text analysis tasks, such as word frequency analysis, sentiment scoring, and topic modeling.

### 2nd Dataset

### 3rd dataset

## Exploratory Data Analysis (EDA) {.tabset .tabset-fade .tabset-pills}

### IMDB Dataset

In this section, we will analyze the distribution of the `sentiment` variable in the IMDB dataset. Understanding the distribution of sentiment is essential for assessing class balance, which can impact model performance. Additionally, we will examine basic descriptive statistics and structural information to ensure that the data aligns with our modeling requirements.

#### Word Frequency Analysis

To understand the most common words in the reviews, we perform a word frequency analysis. This analysis helps identify the most frequent words used in the reviews, providing insights into the language patterns and topics discussed in the dataset.

```{r word frequency analysis, warning=FALSE}
# Calculate word frequencies
word_counts <- tokenized_reviews %>%
  count(word, sort = TRUE)

# Obtener la palabra más frecuente DESPUÉS de eliminar stop words
top_word_no_sw <- word_counts %>%
  anti_join(tibble(word = stopwords("en")), by = "word") %>%
  top_n(1, n) %>%
  pull(word)

# Obtener la palabra más frecuente CON stop words (para comparacion y que se vea el top 1 original)
top_word_with_sw <- word_counts %>%
  top_n(1, n) %>%
  pull(word)


# --- Gráfico 1: Con stop words, destacando ambas palabras ---
word_counts %>%
  top_n(20, n) %>%
  mutate(word = reorder(word, n), # Esto va dentro de mutate
         color = case_when(       # Esto también va dentro de mutate
           word == top_word_no_sw ~ "orange",
           word == top_word_with_sw ~ "red",
           TRUE ~ "blue"
         )) %>% # Cierra mutate() aquí
  ggplot(aes(x = word, y = n, color = color)) +
  geom_segment(aes(xend = word, yend = 0), size = 1.2) +
  geom_point(size = 4) +
  geom_text(aes(label = comma(n), y = n), nudge_y = 50, hjust = -0.3, size=3) +
  scale_color_identity() +
  coord_flip() +
  expand_limits(y=c(0, max(word_counts$n) * 1.1)) +
  labs(title = "Top 20 Most Common Words (with Stop Words)", 
       subtitle = paste(" Most common word (with stopwords):", top_word_with_sw, "(in red)\n",
                        "Most common word (no stopwords):", top_word_no_sw, "(in orange)"),
       x = "Words", y = "Frequency") +
  theme_minimal() +
  theme(text = element_text(size = 10, family = "Montserrat")) + 
  theme(panel.grid.major.y = element_blank(), axis.text.y = element_text(size = 12, family = "Montserrat"), plot.title = element_text(size = 14, face = "bold", family = "Montserrat"), plot.subtitle = element_text(size = 10, face = "italic", family = "Montserrat")) +
  scale_y_continuous(labels = comma)
```

The lollipop chart above displays the most common words in the reviews, sorted by frequency. The visualization provides insights into the language patterns and topics discussed in the dataset.

As we can see in the plot, the most frequent words include common English words like "the," "and," "a," and "of," which are known as stopwords. These words are essential for sentence structure but may not provide meaningful insights for sentiment analysis. We will address this issue by removing stopwords in the next step.

#### Removing Stopwords

Stopwords are common words that do not carry significant meaning in text analysis tasks. Removing stopwords can improve the quality of text analysis by focusing on content words that convey sentiment and meaning. We will remove stopwords from the tokenized reviews using the `anti_join()` function from the `tidytext` package.

```{r removing stopwords, warning=FALSE}
# Load the stopwords dataset
# stop_words <- stopwords::stopwords("en") - this needs to be turned into a dataframe due to the anti_join function
stop_words_df <- tibble(word = stopwords("en"))
# or to a extended list of stopwords
# stop_words_extended <- stop_words %>% filter(lexicon == "snowball") %>% select(word)
```

and then we obtain

```{r word frequency analysis after removing stopwords, warning=FALSE}
max_top_word_with_sw <- word_counts %>%
  anti_join(stop_words_df, by = "word") %>%
  pull(n) %>%
  max()

word_counts %>%
  anti_join(tibble(word = stopwords("en")), by = "word") %>%
  top_n(20, n) %>%
  mutate(word = reorder(word, n),
         color = ifelse(row_number() == 1, "orange", "blue")) %>% # Color condicional
  ggplot(aes(x = word, y = n, color = color)) +
  geom_segment(aes(xend = word, yend = 0), size = 1.5) +
  geom_point(size = 4) +
  geom_text(aes(label = comma(n), y = n), nudge_y = 50, hjust = -0.3, size=3) +
  scale_color_identity() + # Importante: usar scale_color_identity()
  coord_flip() +
  expand_limits(y = c(0, max_top_word_with_sw * 1.1)) + # Usar el nuevo máximo
  labs(title = "Top 20 Most Common Words (with no stopwords)", x = "Words", y = "Frequency") +
  theme_minimal() +
  theme(text = element_text(size = 10, family = "Montserrat")) +
  theme(panel.grid.major.y = element_blank(), axis.text.y = element_text(size = 12, family = "Montserrat"), plot.title = element_text(size = 14, face = "bold", family = "Montserrat"), plot.subtitle = element_text(size = 10, face = "italic", family = "Montserrat")) +
  scale_y_continuous(labels = comma)
```

once we have eliminated the stopwords, we can see the most common words in the reviews are, as we could expect, words like "movie" or "film" which is something obvious, because of the origin of the dataset. Before to proceed with the analysis of the sentiment distribution, we could add one or two more steps to clean the text, like stemming the words or lemmatization of the words.

#### Stemming

Stemming is the process of reducing words to their root form, which helps to group together words with similar meanings. We can apply stemming to the tokenized reviews using the `wordStem()` function from the `SnowballC` package. This step simplifies the text data and reduces the number of unique words, which can improve the performance of text analysis models this process is not always necessary, but it can help to reduce the number of unique words in the dataset from the same root word, for example, "running" and "run" will be reduced to "run" but may reduce words like "better" to "bet" which may not be useful.

```{r stemming, warning=FALSE}
# Stemming the words
word_counts_stem <- tokenized_reviews %>%
  anti_join(stop_words_df, by = "word") %>%
  mutate(word = wordStem(word, language = "en")) %>%
  count(word, sort = TRUE)

print(as_tibble(word_counts_stem), n = 5)
```

and we reduce the number of words from [11 million](#tokenization) to 181,000 words and then we get the next distribution of the words

```{r word frequency analysis after removing stopwords and stemming, warning=FALSE}
max_top_word_with_sw_stem <- word_counts_stem %>%
  pull(n) %>%
  max()

word_counts_stem %>%
  top_n(20, n) %>%
  mutate(word = reorder(word, n),
         color = ifelse(row_number() == 1, "orange", "blue")) %>%
  ggplot(aes(x = word, y = n, color = color)) +
  geom_segment(aes(xend = word, yend = 0), size = 1.5) +
  geom_point(size = 4) +
  geom_text(aes(label = comma(n), y = n), nudge_y = 50, hjust = -0.3, size=3) +
  scale_color_identity() +
  coord_flip() +
  expand_limits(y = c(0, max_top_word_with_sw_stem * 1.1)) +
  labs(title = "Top 20 Most Common Words (Stemmed, No Stopwords)", x = "Words", y = "Frequency") +
  theme_minimal() +
  theme(text = element_text(size = 10, family = "Montserrat"),
        panel.grid.major.y = element_blank(), 
        axis.text.y = element_text(size = 12, family = "Montserrat"), 
        plot.title = element_text(size = 14, face = "bold", family = "Montserrat"), 
        plot.subtitle = element_text(size = 10, face = "italic", family = "Montserrat")) +
  scale_y_continuous(labels = comma)
```

and as we can observe from these results, some words have been modified and even the top 1 word has increased if we compare it with the previous plot. This is because the words have been reduced to their root form, which groups together words with similar roots, so words like "movie" and "movies" are reduced to "movi".

#### Lemmatization

1. **lemmatize_words()**

Lemmatization is another text normalization technique that reduces words to their base form, based on grammatical rules from the dictionary. Unlike stemming, lemmatization considers the context of the word in the sentence and applies morphological analysis to obtain the root form. We can apply lemmatization to the tokenized reviews using the `lemmatize_words()` function from the `textstem` package.

```{r lemmatization, warning=FALSE}
# Lemmatization of the words
word_counts_lemma <- tokenized_reviews %>%
  anti_join(stop_words_df, by = "word") %>%
  mutate(word = textstem::lemmatize_words(word)) %>%
  count(word, sort = TRUE)

print(as_tibble(word_counts_lemma), n = 5)
```

and then we reduce the number of words from [11 million](#tokenization) to 200,000 words and then we get the next distribution of the words

```{r word frequency analysis after removing stopwords and lemmatization, warning=FALSE}
max_top_word_with_sw_lemma <- word_counts_lemma %>%
  pull(n) %>%
  max()

word_counts_lemma %>%
  top_n(20, n) %>%
  mutate(word = reorder(word, n),
         color = ifelse(row_number() == 1, "orange", "blue")) %>%
  ggplot(aes(x = word, y = n, color = color)) +
  geom_segment(aes(xend = word, yend = 0), size = 1.5) +
  geom_point(size = 4) +
  geom_text(aes(label = comma(n), y = n), nudge_y = 50, hjust = -0.3, size=3) +
  scale_color_identity() +
  coord_flip() +
  expand_limits(y = c(0, max_top_word_with_sw_lemma * 1.1)) +
  labs(title = "Top 20 Most Common Words (Lemmatized, No Stopwords)", x = "Lemmas", y = "Frequency") +
  theme_minimal() +
  theme(text = element_text(size = 10, family = "Montserrat"),
        panel.grid.major.y = element_blank(), 
        axis.text.y = element_text(size = 12, family = "Montserrat"), 
        plot.title = element_text(size = 14, face = "bold", family = "Montserrat"), 
        plot.subtitle = element_text(size = 10, face = "italic", family = "Montserrat")) +
  scale_y_continuous(labels = comma)
```

and as we can observe from these results, some words have been modified and even the top 1 word has increased if we compare it with the previous plot. This is because the words have been reduced to their base form, which groups together words with similar roots, so words like "movie" and "movies" are reduced to "movie".

2. **udpipe**

But `lemmatize_words()` is a simple way to lemmatize, another way to lemmatize is using the `udpipe` package, which is a more complex way to lemmatize, but it's more accurate.

Before to proceed with the lemmatization, we need to modify our tokenized_reviews dataframe so we repeat our process from above (this is made to know how review words are related with the sentiment, it means what each review word comes from, so we need to have an id for each word).

```{r preparing dataset, warning=FALSE}
# Tokenization
tokenized_reviews <- df %>%
  mutate(review_id = row_number()) %>% # Adding review_id at beginning
  unnest_tokens(word, review)

# counting words of the tokenized reviews
word_counts <- tokenized_reviews %>%
  count(word, sort = TRUE)

# obtaining the most common word before removing stopwords
top_word_no_sw <- word_counts %>%
  anti_join(tibble(word = stopwords("en")), by = "word") %>%
  top_n(1, n) %>%
  pull(word)

# obtaining the most common word with stopwords
top_word_with_sw <- word_counts %>%
  top_n(1, n) %>%
  pull(word)

# removing stopwords
stop_words_df <- tibble(word = stopwords("en"))

# final dataframe to use udpipe
reviews_text_udpipe <- tokenized_reviews %>%
  anti_join(stop_words_df, by = "word") %>%
  group_by(review_id) %>% 
  summarize(text = paste(word, collapse = " "))
```


**NOTE:THE MODEL LOCATION TO LOAD THE MODEL IN THE NEXT CHUNK OF CODE DEPENDS ON WHERE IT IS LOCALIZED, FOR EXAMPLE, IN MY CASE IT WAS IN THE ROOT DIRECTORY SO I HAD TO USE "../english-ewt-ud-2.5-191206.udpipe" AS MY .Rmd FILE IS IN THE DIRECTORY "notebooks"**

```{r lemmatization_udpipe, warning=FALSE}
# (Download and) Load the udpipe model
# udpipe_download_model(language = "english") # Uncomment to download the model
udmodel <- udpipe_load_model("../english-ewt-ud-2.5-191206.udpipe")

# Lematization with udpipe
lemma_df_udpipe <- udpipe(reviews_text_udpipe$text, udmodel) %>%
  as_tibble() %>%
  filter(upos %in% c("NOUN", "ADJ", "VERB", "ADV")) %>%
  select(lemma) %>%
  group_by(lemma) %>%
  count(sort = TRUE)

print(as_tibble(lemma_df_udpipe), n = 5)
```

This process is more complex than the previous one, but it's more accurate. The output of the lemmatization process is a dataframe with the lemmatized words and their frequency. We can observe that this dataframe is shorter than the previous one, which is due to the lemmatization process that groups together words with similar meanings, we pass from 200,000 words to 169,000 words.

Before to proceed, we need to convert the dataframe to a tibble, due to

- It prevents grouping interference in operations that are not designed to work with grouped data.
- It simplifies the code and makes it more readable.
- It improves efficiency in some cases.

```{r convert lemma_df_udpipe to tibble, warning=FALSE}
lemma_df_udpipe <- lemma_df_udpipe %>%
  ungroup() %>%
  as_tibble()
```

and the distribution of the model 

```{r obtaining the most common word with stopwords and lemmatization, warning=FALSE}
# we first obtain the top 20 of the lemmatized words
top_20_lemmas <- lemma_df_udpipe %>%
  arrange(desc(n)) %>%
  slice(1:20) %>%
  mutate(lemma = factor(lemma, levels = lemma[order(n, decreasing = FALSE)]), # Ordenamiento con factor
         color = ifelse(row_number() == 1, "orange", "blue"))

# Calculate the maximum frequency of the top 20 lemmas
max_top_word_with_sw_lemma_udpipe <- top_20_lemmas %>%
  pull(n) %>%
  max()
```

```{r word frequency analysis after removing stopwords and lemmatization_udpipe, warning=FALSE}
# Crear el gráfico con las palabras lematizadas con udpipe
ggplot(top_20_lemmas, aes(x = lemma, y = n, color = color)) +
  geom_segment(aes(xend = lemma, yend = 0), size = 1.5) +
  geom_point(size = 4) +
  geom_text(aes(label = comma(n), y = n), nudge_y = 50, hjust = -0.3, size=3) +
  scale_color_identity() +
  coord_flip() +
  expand_limits(y = c(0, max_top_word_with_sw_lemma_udpipe * 1.1)) +
  labs(title = "Top 20 Most Common Lemmas (udpipe, No Stopwords)", x = "Lemmas", y = "Frequency") +
  theme_minimal() +
  theme(text = element_text(size = 10, family = "Montserrat"),
        panel.grid.major.y = element_blank(),
        axis.text.y = element_text(size = 12, family = "Montserrat"),
        plot.title = element_text(size = 14, face = "bold", family = "Montserrat"),
        plot.subtitle = element_text(size = 10, face = "italic", family = "Montserrat")) +
  scale_y_continuous(labels = comma)
```

This last execution is more complex due to the use of the model of language which is more accureta than the function `lemmatize_words()` but consummes a lot of resources. Nevertheless it has support in other languages, besides english. Compared with the other lemmatization process, this one has reduced the number of words from 200,000 to 169,000 words, which is a good result, but it's not always necessary to use this process, it depends on the requirements of the project.

#### n-gram analysis

Another way to analyze the text is by using n-grams, which are sequences of words that appear together in the text. We can analyze the most common n-grams in the reviews to identify common phrases or expressions that may provide insights into the sentiment of the reviews, for example, we could have words like "not" and "good" that together could mean a negative sentiment.

1.- **Bigrams**

Bigrams are n-grams of size 2, which represent pairs of words that appear together in the text. We can extract bigrams from the tokenized reviews using the `unnest_ngrams()` function from the `tidytext` package.

```{r}
# Crear bigramas
bigrams <- reviews_text_udpipe %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2) %>%
  separate(bigram, into = c("word1", "word2"), sep = " ") %>%
  filter(!word1 %in% stop_words_df$word, !word2 %in% stop_words_df$word) %>%
  unite(bigram, word1, word2, sep = " ") %>%
  count(bigram, sort = TRUE)

# Ver los bigramas más comunes
print(bigrams, n = 10)
```

In the above chunk of code, we have 

- `unnest_tokens(bigram, text, token = "ngrams", n = 2)` which creates bigrams from the `text` column in `reviews_text_udpipe`, `bigram` is the name of the new column and `token="ngrams"` specifies that we want to create n-grams, with `n=2` indicating bigrams,
- `separate(bigram, into = c("word1", "word2"), sep = " ")` separates the bigrams into two columns `word1` and `word2` based on the space separator, 
- `filter(!word1 %in% stop_words_df$word, !word2 %in% stop_words_df$word)` removes stopwords from the bigrams, 
- `unite(bigram, word1, word2, sep = " ")` combines the two words into a single bigram, and 
- `count(bigram, sort = TRUE)` calculates the frequency of each bigram.

From this procedure, we can obtain phrases like "special effects" or "ever seen" which could be useful to understand the sentiment of the reviews. We can observe that phrases like "one best" or "waste time" are common in the reviews, which may indicate positive and negative sentiments, respectively, while "special effects" or "ive seen" are neutral phrases that may not provide clear sentiment information.

```{r}
# Supongamos que tienes un dataframe llamado 'trigrams' con las columnas 'trigrams' y 'n'
bigrams %>%
  slice_max(n, n = 10) %>% 
  mutate(bigram = reorder(bigram, n), 
         color = ifelse(row_number() == 1, "orange", "blue")) %>% 
  ggplot(aes(x = bigram, y = n, color = color)) +
  geom_segment(aes(xend = bigram, yend = 0), size = 1.5) +
  geom_point(size = 4) +
  geom_text(aes(label = comma(n), y = n), nudge_y = 50, hjust = -0.3, size = 3) +
  scale_color_identity() +
  coord_flip() +
  expand_limits(y = c(0, max(bigrams$n) * 1.1)) +
  labs(title = "Top 10 Most Common Bigrams",
       x = "Bigrams",
       y = "Frequency") +
  theme_minimal() +
  theme(text = element_text(size = 10, family = "Montserrat"),
        panel.grid.major.y = element_blank(),
        axis.text.y = element_text(size = 12, family = "Montserrat"),
        plot.title = element_text(size = 14, face = "bold", family = "Montserrat"),
        plot.subtitle = element_text(size = 10, face = "italic", family = "Montserrat")) +
  scale_y_continuous(labels = comma)
```

In this plot, we can see the most common bigrams in the reviews, sorted by frequency. The visualization provides insights into common phrases and expressions used in the reviews, which may help identify patterns and sentiments in the text data.

2.- **Trigrams**


```{r}
# Crear bigramas
trigrams <- reviews_text_udpipe %>%
  unnest_tokens(trigrams, text, token = "ngrams", n = 3) %>%
  separate(trigrams, into = c("word1", "word2", "word3"), sep = " ") %>%
  filter(!word1 %in% stop_words_df$word, !word2 %in% stop_words_df$word, !word3 %in% stop_words_df$word) %>%
  unite(trigrams, word1, word2, word3, sep = " ") %>%
  count(trigrams, sort = TRUE)

# Ver los bigramas más comunes
print(trigrams, n = 10)
```

in this case, we can see that the most common trigram is "ive ever seen" which is something that could be expected due to the origin of the dataset, but we can see that there are other trigrams like "worst movie ever" or "dont waste time" which could be useful to understand the sentiment of the reviews. 

```{r}
# Supongamos que tienes un dataframe llamado 'trigrams' con las columnas 'trigrams' y 'n'
trigrams %>%
  slice_max(n, n = 10) %>%  # Seleccionar los 10 trigramas más comunes
  mutate(trigrams = reorder(trigrams, n),
         color = ifelse(row_number() == 1, "orange", "blue")) %>%
  ggplot(aes(x = trigrams, y = n, color = color)) +
  geom_segment(aes(xend = trigrams, yend = 0), size = 1.5) +
  geom_point(size = 4) +
  geom_text(aes(label = comma(n), y = n), nudge_y = 10, hjust = -0.3, size = 3) +
  scale_color_identity() +
  coord_flip() +
  expand_limits(y = c(0, max(trigrams$n) * 1.1)) +
  labs(title = "Top 10 Most Common Trigrams",
       x = "Trigrams",
       y = "Frequency") +
  theme_minimal() +
  theme(text = element_text(size = 10, family = "Montserrat"),
        panel.grid.major.y = element_blank(),
        axis.text.y = element_text(size = 12, family = "Montserrat"),
        plot.title = element_text(size = 14, face = "bold", family = "Montserrat"),
        plot.subtitle = element_text(size = 10, face = "italic", family = "Montserrat")) +
  scale_y_continuous(labels = comma)
```

In this plot, we can see the most common trigrams in the reviews, sorted by frequency. The visualization provides insights into common phrases and expressions used in the reviews, which may help identify patterns and sentiments in the text data.

3.- **Tetragrams**

```{r}
# Crear bigramas
tetragrams <- reviews_text_udpipe %>%
  unnest_tokens(tetragrams, text, token = "ngrams", n = 4) %>%
  separate(tetragrams, into = c("word1", "word2", "word3", "word4"), sep = " ") %>%
  filter(!word1 %in% stop_words_df$word, !word2 %in% stop_words_df$word, !word3 %in% stop_words_df$word, !word4 %in% stop_words_df$word) %>%
  unite(tetragrams, word1, word2, word3, word4, sep = " ") %>%
  count(tetragrams, sort = TRUE)

# Ver los bigramas más comunes
print(tetragrams, n = 10)
```

from these results, we have phrases like "one worst movies ever" or "worst movie ever seen" which could be useful to understand the sentiment of the reviews, also these kind of expressions are basically the same but with different words, so we could reduce the number of unique words in the dataset. As we observe, these reviews have the repetition of the word "worst" which suggest that the reviews are negative.

```{r}
# Supongamos que tienes un dataframe llamado 'trigrams' con las columnas 'trigrams' y 'n'
tetragrams %>%
  slice_max(n, n = 10) %>%  # Seleccionar los 10 trigramas más comunes
  mutate(tetragrams = reorder(tetragrams, n),
         color = ifelse(row_number() == 1, "orange", "blue")) %>%
  ggplot(aes(x = tetragrams, y = n, color = color)) +
  geom_segment(aes(xend = tetragrams, yend = 0), size = 1.5) +
  geom_point(size = 4) +
  geom_text(aes(label = comma(n), y = n), nudge_y = 10, hjust = 0.3, size = 3) +
  scale_color_identity() +
  coord_flip() +
  expand_limits(y = c(0, max(tetragrams$n) * 1)) +
  labs(title = "Top 10 Most Common Tetragrams",
       x = "Tetragrams",
       y = "Frequency") +
  theme_minimal() +
  theme(text = element_text(size = 10, family = "Montserrat"),
        panel.grid.major.y = element_blank(),
        axis.text.y = element_text(size = 12, family = "Montserrat"),
        plot.title = element_text(size = 14, face = "bold", family = "Montserrat"),
        plot.subtitle = element_text(size = 10, face = "italic", family = "Montserrat")) +
  scale_y_continuous(labels = comma)
```

In the above plot, we can observe that the most common expressions are similar and seems to express negative sentiments, with phrases like "one worst movies ever" or "worst movie ever seen" being repeated in the reviews. These expressions suggest that the reviews are negative and provide insights into the sentiment of the reviewers.

4.- **Pentagrams**

```{r}
# Crear bigramas
pentagrams <- reviews_text_udpipe %>%
  unnest_tokens(pentagrams, text, token = "ngrams", n = 5) %>%
  separate(pentagrams, into = c("word1", "word2", "word3", "word4", "word5"), sep = " ") %>%
  filter(!word1 %in% stop_words_df$word, !word2 %in% stop_words_df$word, !word3 %in% stop_words_df$word, !word4 %in% stop_words_df$word, !word5 %in% stop_words_df$word) %>%
  unite(pentagrams, word1, word2, word3, word4, word5, sep = " ") %>%
  count(pentagrams, sort = TRUE)

# Ver los bigramas más comunes
print(pentagrams, n = 10)
```

these results seems similar to the tetragrams, with expressions like "one worst movies ever seen" or "worst movie ive ever seen" and when we compare with the tetragrams, we can observe that these expressions are basically the same, but with a redundant word like "seen" or "ive" which could be removed to reduce the number of unique words in the dataset.

```{r}
# Supongamos que tienes un dataframe llamado 'trigrams' con las columnas 'trigrams' y 'n'
pentagrams %>%
  slice_max(n, n = 10) %>%  # Seleccionar los 10 trigramas más comunes
  mutate(pentagrams = reorder(pentagrams, n),
         color = ifelse(row_number() == 1, "orange", "blue")) %>%
  ggplot(aes(x = pentagrams, y = n, color = color)) +
  geom_segment(aes(xend = pentagrams, yend = 0), size = 1.5) +
  geom_point(size = 4) +
  geom_text(aes(label = comma(n), y = n), nudge_y = 3, hjust = -0.3, size = 3) +
  scale_color_identity() +
  coord_flip() +
  expand_limits(y = c(0, max(pentagrams$n) * 1.1)) +
  labs(title = "Top 10 Most Common Pentagrams",
       x = "Pentagrams",
       y = "Frequency") +
  theme_minimal() +
  theme(text = element_text(size = 10, family = "Montserrat"),
        panel.grid.major.y = element_blank(),
        axis.text.y = element_text(size = 12, family = "Montserrat"),
        plot.title = element_text(size = 14, face = "bold", family = "Montserrat"),
        plot.subtitle = element_text(size = 10, face = "italic", family = "Montserrat")) +
  scale_y_continuous(labels = comma)
```

In the above plot, we can observe that the most common expressions are similar to the tetragrams, with phrases like "one worst movies ever seen" or "worst movie ive ever seen" being repeated in the reviews. These expressions are not likely to provide extra information for our dataset, so we could stop at the tetragrams analysis due to the redundancy of the expressions.

### Sentiment Distribution

We continue with the analysis of the sentiment distribution, which is important to understand the balance of the dataset and the distribution of positive and negative reviews.

1.- **By word**

- **No stopwords**

Starting from the `tokenized_reviews` dataframe, we can count the frequency of each word and visualize the distribution of the sentiment by word. This analysis helps to identify the most common words associated with positive and negative sentiments in the reviews.

```{r counting sentiments by word (no stopwords), warning=FALSE}
# Filtrar stopwords y palabras con caracteres extraños
word_counts_sentiment <- tokenized_reviews %>%
  anti_join(stop_words_df, by = "word") %>%  # Quitar stopwords
  filter(str_detect(word, "^[a-zA-Z]+$")) %>%  # Solo palabras alfabéticas
  group_by(word, sentiment) %>%
  summarise(count = n(), .groups = "drop") %>%  # Contar palabras por sentimiento
  pivot_wider(names_from = sentiment, values_from = count, values_fill = 0) %>%  # Convertir en columnas
  rename(negative = negative, positive = positive) %>%
  filter(negative + positive > 10) %>%  # Filtrar palabras con muy baja frecuencia
  arrange(desc(negative + positive))  # Ordenar por frecuencia total

# Verificar resultados
print(as_tibble(word_counts_sentiment), n = 10)
```
47490+36019= 83509

in the above chunk of code, we have the top 10 words with their sentiment distribution, we can see that the word "movie" has a higher proportion of negative sentiment, while the word "film" has a higher proportion of positive sentiment. This analysis provides insights into the sentiment distribution of the most common words in the reviews.

<!-- # ```{r lollipot plot of the sentiment distribution by word (no stopwords), warning=FALSE} -->
<!-- # word_counts_sentiment %>% -->
<!-- #   filter(total > 20) %>%  # Filtrar palabras con suficiente frecuencia -->
<!-- #   mutate(sentiment = ifelse(positive > negative, "positive", "negative"), -->
<!-- #          word = reorder(word, total)) %>% -->
<!-- #   slice_max(total, n = 10) %>%  -->
<!-- #   ggplot(aes(x = word, y = total, color = sentiment)) + -->
<!-- #   geom_segment(aes(xend = word, yend = 0), size = 1.5) + -->
<!-- #   geom_point(size = 4) + -->
<!-- #   coord_flip() + -->
<!-- #   labs(title = "Top 10 Most Frequent Words by Sentiment", -->
<!-- #        x = "Words", -->
<!-- #        y = "Frequency") + -->
<!-- #   scale_color_manual(values = c("positive" = "blue", "negative" = "red")) + -->
<!-- #   theme_minimal() -->
<!-- # ``` -->
<!-- but this has a problem, we have not considered that some reviews are larger than other reviews, so we need to consider the length of the reviews to have a better understanding of the sentiment distribution. For example, what if the word "movie" appears in a review with 100 words with negative sentiment, while the word "film" appears in a review with 10 words with positive sentiment, this could affect the sentiment distribution of the words. To do this, we could calculate the relative frequency of each word by the length of the reviews. -->

<!-- $$ -->
<!-- \text{Relative Frequency} = \frac{\text{Frequency of the word}}{\text{Total number of words in the reviews}} -->
<!-- $$ -->

<!-- ```{r counting sentiments by word using (no stopwords) relative to their length, warning=FALSE} -->
<!-- # Contar cuántas veces aparece cada palabra en cada sentimiento -->
<!-- word_counts_sentiment <- tokenized_reviews %>% -->
<!--   anti_join(stop_words_df, by = "word") %>% -->
<!--   mutate(word = wordStem(word, language = "en")) %>% -->
<!--   count(word, sentiment) %>% -->
<!--   pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %>%  # Consolidar en una sola fila -->
<!--   rename(negative = negative, positive = positive) -->

<!-- # Calcular la frecuencia relativa de cada palabra en su categoría -->
<!-- word_counts_sentiment <- word_counts_sentiment %>% -->
<!--   mutate(freq_negative = negative / sum(negative), -->
<!--          freq_positive = positive / sum(positive)) -->

<!-- # Mostrar las 5 palabras más frecuentes con frecuencia normalizada -->
<!-- print(as_tibble(word_counts_sentiment), n = 5) -->
<!-- ``` -->

<!-- in this code chunk, we have the top 5 words with their sentiment distribution relative to the length of the reviews, we can see that the word "movie" has a higher proportion of negative sentiment, while the word "film" has a higher proportion of positive sentiment. This analysis provides insights into the sentiment distribution of the most common words in the reviews, considering the length of the reviews. -->



<!-- - *Stemming* -->


<!-- ```{r counting sentiments by word using (no stopwords), warning=FALSE} -->
<!-- # Stemming y conteo por sentimiento -->
<!-- word_counts_stem_sentiment <- tokenized_reviews %>% -->
<!--   anti_join(stop_words_df, by = "word") %>% -->
<!--   mutate(word = wordStem(word, language = "en")) %>% -->
<!--   count(word, sentiment, sort = TRUE) %>% -->
<!--   pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %>% -->
<!--   rename(negative = negative, positive = positive) %>% -->
<!--   mutate(total = positive + negative, -->
<!--          prop_positive = positive / total, -->
<!--          prop_negative = negative / total) -->

<!-- # Mostrar las 5 palabras más frecuentes -->
<!-- print(as_tibble(word_counts_stem_sentiment), n = 5) -->
<!-- ``` -->



<!-- #### Term Frequency - Inverse document frequency (TF-IDF) Analysis -->

